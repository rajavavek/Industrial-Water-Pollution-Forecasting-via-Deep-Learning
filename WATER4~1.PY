# -*- coding: utf-8 -*-
"""Water4.0-An-Industrial-Water-Pollution-Forecasting-Using-Machine-Learning .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gwKIxa8GgZeVTp9ThuIg3y90YwSI5aXn

# **Water4.0-An-Industrial-Water-Pollution-Forecasting-Using-Machine-Learning**
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

df=pd.read_csv(r'D:\Datasets\water_potability.csv')
df.head()

"""# Exploratory Data Analysis"""

df.shape

df.isnull().sum()

df.info()

df.describe()

df['Sulfate'].mean()

df.fillna(df.mean(), inplace=True)
df.head()

df.isnull().sum()

df.info()

df.describe()

df.Potability.value_counts()

df.Potability.value_counts().plot(kind="bar", color=["brown", "salmon"])
plt.show()

sns.distplot(df['ph'])

df.hist(figsize=(14,14))
plt.show()

sns.pairplot(df,hue='Potability')

sns.scatterplot(df['Hardness'],df['Solids'])

sns.scatterplot(df['ph'],df['Potability'])

# create a correlation heatmap
sns.heatmap(df.corr(),annot=True, cmap='terrain', linewidths=0.1)
fig=plt.gcf()
fig.set_size_inches(8,6)
plt.show()

df.boxplot(figsize=(14,7))

df['Solids'].describe()



"""# Partitioning"""

X = df.drop('Potability',axis=1)

Y= df['Potability']

from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size= 0.2, random_state=101,shuffle=True)

Y_train.value_counts()

Y_test.value_counts()

"""# Normalization"""

#from sklearn.preprocessing import StandardScaler
#sc=StandardScaler()

#X_train = sc.fit_transform(X_train)
#X_test = sc.transform(X_test)

"""# Model Building

# DT
"""

from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score,confusion_matrix,precision_score
dt=DecisionTreeClassifier(criterion= 'gini', min_samples_split= 10, splitter= 'best')
dt.fit(X_train,Y_train)

prediction=dt.predict(X_test)
accuracy_dt=accuracy_score(Y_test,prediction)*100
accuracy_dt

print("Accuracy on training set: {:.3f}".format(dt.score(X_train, Y_train)))
print("Accuracy on test set: {:.3f}".format(dt.score(X_test, Y_test)))



accuracy_score(prediction,Y_test)

print("Feature importances:\n{}".format(dt.feature_importances_))

confusion_matrix(prediction,Y_test)

"""# Prediction on only one set of data"""

X_DT=dt.predict([[5.735724, 158.318741,25363.016594,7.728601,377.543291,568.304671,13.626624,75.952337,4.732954]])

X_DT

"""# KNN"""

from sklearn.neighbors import KNeighborsClassifier

knn=KNeighborsClassifier(metric='manhattan', n_neighbors=22)
knn.fit(X_train,Y_train)

prediction_knn=knn.predict(X_test)
accuracy_knn=accuracy_score(Y_test,prediction_knn)*100
print('accuracy_score score     : ',accuracy_score(Y_test,prediction_knn)*100,'%')

confusion_matrix(prediction,Y_test)

"""# Hyperparameter Tuning / Model Optimization

# DT HPT
"""

dt.get_params().keys()

#example of grid searching key hyperparametres for logistic regression
from sklearn.model_selection import RepeatedStratifiedKFold
from sklearn.model_selection import GridSearchCV

# define models and parameters
model = DecisionTreeClassifier()
criterion = ["gini", "entropy"]
splitter = ["best", "random"]
min_samples_split = [2,4,6,8,10]

# define grid search
grid = dict(splitter=splitter, criterion=criterion, min_samples_split=min_samples_split)
cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)
grid_search_dt = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv,
                           scoring='accuracy',error_score=0, iid=True)
grid_search_dt.fit(X_train, Y_train)

# summarize results
print(f"Best: {grid_search_dt.best_score_:.3f} using {grid_search_dt.best_params_}")
means = grid_search_dt.cv_results_['mean_test_score']
stds = grid_search_dt.cv_results_['std_test_score']
params = grid_search_dt.cv_results_['params']

for mean, stdev, param in zip(means, stds, params):
    print(f"{mean:.3f} ({stdev:.3f}) with: {param}")

print("Training Score:",grid_search_dt.score(X_train, Y_train)*100)
print("Testing Score:", grid_search_dt.score(X_test, Y_test)*100)

from sklearn.metrics import  make_scorer
from sklearn.model_selection import cross_val_score

def classification_report_with_accuracy_score(Y_test, y_pred2):
    print (classification_report(Y_test, y_pred2)) # print classification report
    return accuracy_score(Y_test, y_pred2) # return accuracy score


nested_score = cross_val_score(grid_search_dt, X=X_train, y=Y_train, cv=cv,
               scoring=make_scorer(classification_report_with_accuracy_score))
print (nested_score)

dt_y_predicted = grid_search_dt.predict(X_test)
dt_y_predicted

grid_search_dt.best_params_

dt_grid_score=accuracy_score(Y_test, dt_y_predicted)
dt_grid_score

confusion_matrix(Y_test, dt_y_predicted)

"""# KNN HPT"""

from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import RepeatedStratifiedKFold
from sklearn.model_selection import GridSearchCV

# define models and parameters
model = KNeighborsClassifier()
n_neighbors = range(1, 31)
weights = ['uniform', 'distance']
metric = ['euclidean', 'manhattan', 'minkowski']

# define grid search
grid = dict(n_neighbors=n_neighbors,weights=weights,metric=metric)
cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=1, random_state=1)
grid_search_knn = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv,
                           scoring='accuracy',error_score=0, iid=True)
grid_search_knn.fit(X_train, Y_train)

# summarize results
print(f"Best: {grid_search_knn.best_score_:.3f} using {grid_search_knn.best_params_}")
means = grid_search_knn.cv_results_['mean_test_score']
stds = grid_search_knn.cv_results_['std_test_score']
params = grid_search_knn.cv_results_['params']

for mean, stdev, param in zip(means, stds, params):
    print(f"{mean:.3f} ({stdev:.3f}) with: {param}")

from sklearn.metrics import  make_scorer
from sklearn.model_selection import cross_val_score

def classification_report_with_accuracy_score(Y_test, y_pred2):
    print (classification_report(Y_test, y_pred2)) # print classification report
    return accuracy_score(Y_test, y_pred2) # return accuracy score


nested_score = cross_val_score(grid_search_knn, X=X_train, y=Y_train, cv=cv,
               scoring=make_scorer(classification_report_with_accuracy_score))
print (nested_score)

knn_y_predicted = grid_search_knn.predict(X_test)

knn_y_predicted

knn_grid_score=accuracy_score(Y_test, knn_y_predicted)

knn_grid_score

grid_search_knn.best_params_

confusion_matrix(Y_test, knn_y_predicted)



"""# Prediction on only one set of data"""

X_KNN=knn.predict([[5.735724, 158.318741,25363.016594,7.728601,377.543291,568.304671,13.626624,75.952337,4.732954]])

X_KNN